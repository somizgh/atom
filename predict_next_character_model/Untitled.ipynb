{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 텐서플로우의 버전을 확인해봅니다.\n",
    "print(tf.__version__)\n",
    "\n",
    "# numpy 를 np 라는 이름으로 축약해서 import 합니다.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_file = tf.keras.utils.get_file('input.txt', 'https://raw.githubusercontent.com/greentec/greentec.github.io/master/public/other/data/koreanhiphop/input.txt')\n",
    "path_to_file2 = \"./sample.txt\"\n",
    "path_to_file2 = \"E:/Ai_projects_data/atom_data/ko_wiki_extraction/AA/wiki_00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1831099 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file2, 'r',encoding=\"UTF-8\").read()#.decode(encoding='utf-8')\n",
    "# 빠른 테스트를 위해 데이터 크기를 1/5 로 줄이는 옵션입니다. \n",
    "# 더 큰 크기로 테스트하면 더 좋은 결과가 나오겠지만 epoch 당 학습 시간이 많이 필요합니다.\n",
    "# text = text[:len(text)//5]\n",
    "# 텍스트가 총 몇 자인지 확인합니다.\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이고, 양식적이라 여겨져서, 조지 발란신같은 젊은 안무가가 발레 루스와 함께 큰 진보를 했지만, 첫 번째 몇 시즌에서 절대적인 성공을 거의 얻지 못했다\n",
      "\n",
      "ㅇㅣㄱㅗ, ㅇㅑㅇㅅㅣㄱㅈㅓㄱㅇㅣㄹㅏ ㅇㅕㄱㅕㅈㅕㅅㅓ, ㅈㅗㅈㅣ ㅂㅏㄹㄹㅏㄴㅅㅣㄴㄱㅏㅌㅇㅡㄴ ㅈㅓㄻㅇㅡㄴ ㅇㅏㄴㅁㅜㄱㅏㄱㅏ ㅂㅏㄹㄹㅔ ㄹㅜㅅㅡㅇㅘ ㅎㅏㅁㄲㅔ ㅋㅡㄴ ㅈㅣㄴㅂㅗㄹㅡㄹ ㅎㅐㅆㅈㅣㅁㅏㄴ, ㅊㅓㅅ ㅂㅓㄴㅉㅐ ㅁㅕㅊ ㅅㅣㅈㅡㄴㅇㅔㅅㅓ ㅈㅓㄹㄷㅐㅈㅓㄱㅇㅣㄴ ㅅㅓㅇㄱㅗㅇㅇㅡㄹ ㄱㅓㅇㅢ ㅇㅓㄷㅈㅣ ㅁㅗㅅㅎㅐㅆㄷㅏ\n"
     ]
    }
   ],
   "source": [
    "import jamotools\n",
    "# 한글과 영어가 같이 있는 부분을 임의로 발췌했습니다.\n",
    "s = text[3008:3092]\n",
    "print(s)\n",
    "print()\n",
    "# 한글 텍스트를 자모 단위로 분리해줍니다. 영어에는 영향이 없습니다.\n",
    "s_split = jamotools.split_syllables(s)\n",
    "print(s_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이고, 양식적이라 여겨져서, 조지 발란신같은 젊은 안무가가 발레 루스와 함께 큰 진보를 했지만, 첫 번째 몇 시즌에서 절대적인 성공을 거의 얻지 못했다\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "s2 = jamotools.join_jamos(s_split)\n",
    "print(s2)\n",
    "print(s == s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 unique characters\n"
     ]
    }
   ],
   "source": [
    "text_jamo = jamotools.split_syllables(text)\n",
    "# 자모 단위 텍스트에 존재하는 unique character 를 set 을 이용해서 뽑아내고, sorted 로 정렬합니다.\n",
    "# 이 unique charater 를 보통 vocabulary 라고 합니다.\n",
    "vocab = sorted(set(text_jamo))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab character 를 숫자로 맵핑하고, 반대도 실행합니다.\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text_jamo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '#' :   4,\n",
      "  '$' :   5,\n",
      "  '%' :   6,\n",
      "  '&' :   7,\n",
      "  \"'\" :   8,\n",
      "  '(' :   9,\n",
      "  ')' :  10,\n",
      "  '*' :  11,\n",
      "  '+' :  12,\n",
      "  ',' :  13,\n",
      "  '-' :  14,\n",
      "  '.' :  15,\n",
      "  '/' :  16,\n",
      "  '0' :  17,\n",
      "  '1' :  18,\n",
      "  '2' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# char2idx 의 일부를 알아보기 쉽게 print 해봅니다.\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ㅏ, ㅎㅜㅇㅝㄴㅈㅏ, ㅂㅏㄹㄹㅔ ㅎㅡㅇㅎㅐ' ---- characters mapped to int ---- > [302  13   1 301 315 294 316 276 295 302  13   1 289 302 281 281 307   1\n",
      " 301 320 294 301 303]\n"
     ]
    }
   ],
   "source": [
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text_jamo[210:233]), text_as_int[210:233]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(29, shape=(), dtype=int32)\n",
      "<\n",
      "tf.Tensor(68, shape=(), dtype=int32)\n",
      "d\n",
      "tf.Tensor(79, shape=(), dtype=int32)\n",
      "o\n",
      "tf.Tensor(67, shape=(), dtype=int32)\n",
      "c\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text_jamo)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(i)\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<doc id=\"88201\" url=\"https://ko.wikipedia.org/wiki?curid=88201\" title=\"ㅅㅔㄹㅡㄱㅔㅇㅣ ㄷㅑㄱㅣㄹㄹㅔㅍㅡ\">\\nㅅㅔㄹㅡㄱㅔㅇㅣ '\n",
      "'ㄷㅑㄱㅣㄹㄹㅔㅍㅡ\\nㅅㅔㄹㅡㄱㅔㅇㅣ ㅍㅏㅂㅡㄹㄹㅗㅂㅣㅊㅣ ㄷㅑㄱㅣㄹㄹㅔㅍㅡ(, 1872ㄴㅕㄴ 3ㅇㅝㄹ 31ㅇㅣㄹ ~ 1929ㄴㅕㄴ 8ㅇㅝㄹ 19ㅇㅣㄹ)ㄴㅡㄴ ㄹㅓㅅㅣㅇㅏㅇㅢ ㅁㅣㅅㅜㄹ'\n",
      "' ㅍㅕㅇㄹㅗㄴㄱㅏ, ㅎㅜㅇㅝㄴㅈㅏ, ㅂㅏㄹㄹㅔ ㅎㅡㅇㅎㅐㅇㅈㅜ, ㄱㅡㄹㅣㄱㅗ ㅁㅏㄶㅇㅡㄴ ㅇㅠㅁㅕㅇㅎㅏㄴ ㅁㅜㅇㅛㅇㅅㅜㅇㅘ ㅇㅏㄴㅁㅜㄱㅏㄷㅡㄹㅇㅣ ㄷㅟㅇㅔ ㅁㅕㅇㅅㅓㅇㅇㅡㄹ ㅇㅓㄷㄱ'\n",
      "'ㅔ ㄷㅚㄴ ㅂㅏㄹㄹㅔ ㄹㅜㅅㅡㅇㅢ ㅅㅓㄹㄹㅣㅂㅈㅏㅇㅣㄷㅏ.\\nㅅㅔㄹㅡㄱㅔㅇㅣ ㄷㅑㄱㅣㄹㄹㅔㅍㅡㄴㅡㄴ ㅈㅔㅈㅓㅇ ㅁㅏㄹㄱㅣㅇㅔ ㄷㅏㄷㅏㄹㅡㄹ ㅁㅜㄹㅕㅂ, ㄹㅓㅅㅣㅇㅏ ㅍㅔㄹㅡㅁㅇㅢ ㅂㅜ'\n",
      "'ㅇㅠㅎㅏㄴ ㅈㅣㅂㅇㅏㄴㅇㅔㅅㅓ ㅌㅐㅇㅓㄴㅏㅆㄷㅏ. ㄱㅡㄴㅡㄴ ㅅㅜㄷㅗㄹㅗ ㅂㅗㄴㅐㅈㅕㅅㅓ ㅅㅏㅇㅌㅡㅍㅔㅌㅔㄹㅡㅂㅜㄹㅡㅋㅡ ㄷㅐㅎㅏㄱㅇㅔㅅㅓ ㅂㅓㅂㄹㅠㄹㅇㅡㄹ ㄱㅗㅇㅂㅜㅎㅏㄱㅔ ㄷㅚㅇ'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '<doc id=\"88201\" url=\"https://ko.wikipedia.org/wiki?curid=88201\" title=\"ㅅㅔㄹㅡㄱㅔㅇㅣ ㄷㅑㄱㅣㄹㄹㅔㅍㅡ\">\\nㅅㅔㄹㅡㄱㅔㅇㅣ'\n",
      "Target data: 'doc id=\"88201\" url=\"https://ko.wikipedia.org/wiki?curid=88201\" title=\"ㅅㅔㄹㅡㄱㅔㅇㅣ ㄷㅑㄱㅣㄹㄹㅔㅍㅡ\">\\nㅅㅔㄹㅡㄱㅔㅇㅣ '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 29 ('<')\n",
      "  expected output: 68 ('d')\n",
      "Step    1\n",
      "  input: 68 ('d')\n",
      "  expected output: 79 ('o')\n",
      "Step    2\n",
      "  input: 79 ('o')\n",
      "  expected output: 67 ('c')\n",
      "Step    3\n",
      "  input: 67 ('c')\n",
      "  expected output: 1 (' ')\n",
      "Step    4\n",
      "  input: 1 (' ')\n",
      "  expected output: 73 ('i')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           513536    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 2006)          2056150   \n",
      "=================================================================\n",
      "Total params: 6,507,990\n",
      "Trainable params: 6,507,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab), embedding_dim,\n",
    "                              batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 2006) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.optimizers.Adam(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 모델의 체크포인트가 저장될 디렉토리 이름입니다.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# 체크포인트 파일은 아래에서 지정하는 ckpt_{epoch} 형태로 접두사를 달게 됩니다. 즉 ckpt_5, ckpt_10, ... 이 됩니다.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# 체크포인트 콜백은 model.fit 을 실행할 때 호출됩니다. period 는 저장 주기입니다.\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    "    period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 549 steps\n",
      "Epoch 1/10\n",
      "549/549 - 44s - loss: 2.3734\n",
      "Epoch 2/10\n",
      "549/549 - 41s - loss: 1.9201\n",
      "Epoch 3/10\n",
      "549/549 - 41s - loss: 1.8114\n",
      "Epoch 4/10\n",
      "549/549 - 40s - loss: 1.7548\n",
      "Epoch 5/10\n",
      "549/549 - 40s - loss: 1.7153\n",
      "Epoch 6/10\n",
      "549/549 - 41s - loss: 1.6872\n",
      "Epoch 7/10\n",
      "549/549 - 41s - loss: 1.6650\n",
      "Epoch 8/10\n",
      "549/549 - 40s - loss: 1.6451\n",
      "Epoch 9/10\n",
      "549/549 - 39s - loss: 1.6285\n",
      "Epoch 10/10\n",
      "549/549 - 39s - loss: 1.6134\n"
     ]
    }
   ],
   "source": [
    "# 모델을 실제로 학습시킵니다.\n",
    "# 진행 막대를 표시하지 않기 위해서 verbose=2 로 설정합니다. (0 = 표시없음, 1 = 진행막대 표시)\n",
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 위에서 history 라는 이름으로 저장했던 변수에 있는 loss 정보를 꺾은선 그래프로 그립니다.\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "# plt.show() 함수를 호출해야 위에서 그린 그래프가 출력에 나타납니다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6134416364364068 9\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            513536    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 2006)           2056150   \n",
      "=================================================================\n",
      "Total params: 6,507,990\n",
      "Trainable params: 6,507,990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab), embedding_dim,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# 최신의 체크포인트에서 weights 를 불러옵니다.\n",
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# 가장 loss 가 낮은 weights 를 불러옵니다.\n",
    "min_loss = min(history.history['loss'])\n",
    "index = history.history['loss'].index(min_loss)\n",
    "print(min_loss, index)\n",
    "gen_model.load_weights('./training_checkpoints/ckpt_' + str(index+1))\n",
    "\n",
    "# build() 함수로 모델을 사용할 수 있도록 만듭니다. build() 함수를 사용할 때는 인수로 input_shape 을 같이 써줘야 합니다.\n",
    "gen_model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# gen_model의 구조를 출력합니다.\n",
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated_jamo = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated_jamo.append(idx2char[predicted_id])\n",
    "      \n",
    "  text_generated = jamotools.join_jamos(''.join(text_generated_jamo))\n",
    "\n",
    "  return (jamotools.join_jamos(start_string) + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잠은 안오고 피로무로 줄가 썼는데, 그 1권에서, 한국화된다. 떠항품 싸쿰 관련은 류장으로 쓰이는 연재로 남학생위 황제(네토를 멸의 산동에 있겠다. 또한 자버작시코, 집원으로 SBS, New Gerle intar ungensang hankexpand, Davorlnust irler adia.org/wikipedia.org/wiki?curid=88566\" title=\"하무〉의 사실》에 지배층관도 수상방치에서, 신피로 욘성후는 태안 해병사!ㅃ里면 악시코에서 민(覇漢), 《신의리주]로 1분이라노라 기완이 춛었으나 그리스투에서 주파 습스의 수 있었지만 변지를 끄저본으로써 당지를 이늡하 물리탄스와 뮤양상의 지제데 ㅆ는 서울과 거태수일건의 노래로 독력하였다.\n",
      "가도문니 문쟁콘사키아군\n",
      "863만부 양품은 “덕의해 불표되어 있었다.\n",
      "1189년에는 아울이 능도체 강화물리면 등을 그해서 붓머구며, 존 등과 견보항이산이다.\n",
      "9월 1일, 토면에 있게 된다. 약 3랴튼 대표’, 이료장에서는 지채를 일전하지 모쇼수와 눈 구술릭서키의 조정사관 등 172어국적이며, 부사를 번교 선고부, 폄위원회에도 돌아났다.\n",
      "</doc>\n",
      "<doc id=\"87439\" u\n"
     ]
    }
   ],
   "source": [
    "# print(generate_text(gen_model, start_string=u\"ㄴㅐㄱㅏ ㅈㅔㅇㅣㄹ \"))\n",
    "print(generate_text(gen_model, start_string=u\"ㅈㅏㅁㅇㅡㄴ ㅇㅏㄴㅇㅗㄱㅗ \"))\n",
    "# print(generate_text(gen_model, start_string=u\"ㅇㅗㄴㅡㄹ ㅂㅏㅁㅇㅡㄴ \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
